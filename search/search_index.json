{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-omnibenchmark-documentation","title":"Welcome to Omnibenchmark documentation","text":"<p>Omnibenchmark is a benchmark project that aims to provide community-driven, modular, extensible and always up-to-date benchmarks.</p> <p>It is based on the Renku project, an open and collaborative data analysis platform.</p> <p>The framework connects data, methods and metrics repositories (a.k.a. modules), that can be flexibly extended by any community member.</p> <p>If you are interested in contributing a new method, dataset or metric, follow the documentation from the <code>Getting started</code> section, where you can learn more about how omnibenchmark works and how to extend it with a new module.</p> <p>If you are interested in exploring one of the existing benchmarks and the latest results, you can directly jump to the results of our evaluations in the <code>Output</code> section.</p> <p></p>"},{"location":"01_landing/","title":"Motivation","text":"<p>Benchmarking is a critical step for the development of bioinformatic methods and provides important insights for their application.</p> <p>The current benchmarking scheme has many limitations:</p> <ul> <li> <p>It is a snapshot of the available methods at a certain time point and often already outdated when published.</p> </li> <li> <p>Comparison of benchmarks is challenging: different procedures, different datasets, different evaluation criteria, etc.</p> </li> <li> <p>All of the above can lead to different conclusions among benchmarks made at different time points or at different groups.</p> </li> </ul>"},{"location":"01_landing/#concept","title":"Concept","text":"<p>Omnibenchmark is a modular and extensible framework based on a free open-source analytic platform, Renku, to offer a continuous and open community benchmarking system.</p> <p>The framework consists of data, method and metric repositories (or \u201cmodules\u201d) that share data components and are tracked via a knowledge graph from the Renku system. The results can be displayed in an interactive dashboard to be openly explored by anyone looking for recommendations of tools. New data, methods or metrics can be added by the community to extend the benchmark.</p> <p>Some key features of our benchmarking framework:</p> <ul> <li> <p>Periodical updates of the benchmark to provide up-to-date results</p> </li> <li> <p>Easy extensibility through templates for data, methods or metrics</p> </li> <li> <p>Following FAIR principles by using software containers, an integration with Gitlab and full provenance tracking (inputs, workflows and generated files)</p> </li> <li> <p>Flexibility to work with different benchmarking structures, topics and programming languages.</p> </li> </ul>"},{"location":"01_landing/#prototypes","title":"Prototypes","text":"<p>We are currently building multiple prototypes for community-based benchmarking of single cell batch correction methods. </p> <p>The list of current benchmarks and their results can be explored on the official Omnibenchmark website: </p> <p>http://omnibenchmark.org/</p>"},{"location":"about/","title":"Indetonsusque loca est epulas","text":""},{"location":"about/#calidi-seu","title":"Calidi seu","text":"<p>Lorem markdownum, nunc aret fragorem sorori, dea antemnas sinat, Philippi, auctor, adfixa! Aere Venulus, me dubitare fronte peregrina feruntur et iussit resolvite? Nocte cervix truncos recessit, apri his fatemur timori Tantalis, et haec metum clipeum.</p>"},{"location":"about/#non-nec-comitatur-palmae-redolentia-tamen-aut","title":"Non nec comitatur palmae redolentia tamen aut","text":"<p>Queri voce oblivia subito, ex timentes utque: ferox diligitur candidus veros. Vixque nostri nec est inpulsos cupiuntque in color, terris mihi oves gloria comae aut Salmaci? Inde causa tantos toro: non imo ab atque; abstrahit intexere. Saxum de totaque certe fecit nive triplici, si nam cum meo feroci diu culpa canibusve magna.</p>"},{"location":"about/#sine-fatemur-versa-perdite-est-isto-trabes","title":"Sine fatemur versa perdite est isto trabes","text":"<p>Facundia dixit huic nefasque decimo suae dixit non. Rectum dumque acies qua, dixerat, fitque, tali tuum bellica!</p> <p>Belua poenaededidit lecti ferunt fatale mutentur sacerdos alta, ab nec dis. Nam et Dianae vitare Numam, esse cum! Qui et Lycaon omnes: dixi oblita?</p>"},{"location":"about/#olorinis-summum","title":"Olorinis summum","text":"<p>Colunt breviter Saturnia Threicius motasse aestus tibi obliquo maesto caeruleaeque alti, qui cum vias tenet pelagi noctem? Uni axe ossa, laesi, sed ducere tantum caducas atricolor vicinia pervia!</p> <ol> <li>Veteris abit iussae ictus cohaesit</li> <li>Alter diro Scyllae suoque superat nullos Cumarum</li> <li>Plangoris prohibebant deferre capillis</li> <li>Ponit ante nemus</li> <li>Ait parte quem Hymenaeus</li> </ol> <p>Referrem per mihi spiritus, praecipites Somni quoque, oppositoque. Cum occuluit Iovem ire: pulcherrime neque frondes vultumque anili, sic habet sustulerat quam ad nymphae Haemonias ostendit! Lapis quaerens potitur carpitur iam, terra ima Autolycus Aegaeo, Tegeaea et! Etiam nitidum transire Bromiumque vestrum; thalami enim lateri profundum aconita nec!</p>"},{"location":"01_getting_started/","title":"Getting started","text":"<p>Depending on your role, navigate to the corresponding section </p> <ul> <li> <p>Module contributor: you would like to submit a new dataset, method or metric ? The next section will explain you how to create and submit a new module to an Omnibenchmark. </p> </li> <li> <p>Benchmarker: you have the design for a new benchmark that you would like to port on Omnibenchmark ? The second section will show you how to configure and populate a benchmark on our system. </p> </li> <li> <p>Method user: you would like to access the lattest results of an Omnibenchmark ? The third section will redirect you to shiny apps that will allow you to explore our benchmarks. </p> </li> </ul>"},{"location":"01_getting_started/01_module_contr/","title":"Module contributor","text":"<p>This section is dedicated for module contributors. It will showcase how to add a dataset, a method or a metric to an existing Omnibenchmark. </p> <p>This section will not show you how to create/ setup an Omnibenchmark (see the <code>Benchmarker</code> section) and how to visualize and interpret the evaluation results (see the <code>Method user</code> section).</p> <p>Before starting, make sure that you have;</p> <ol> <li> <p>Created an account to access Renku;</p> </li> <li> <p>Requested an access to the Omnibenchmark Gitlab group (<code>Request Access</code> button. If you don't see it, you were already granted access to it);</p> </li> <li> <p>A script for importing data, or a wrapper of a method or a metric.</p> </li> </ol>"},{"location":"01_getting_started/01_module_contr/config_yaml/","title":"The config.yaml file","text":"<p>Usually all specific information about a benchmark project can be specified in a <code>config.yaml</code> file. Below we show an example with all standard fields and explanations to them. Many fields are optional and do not apply to all modules. All unneccessary fields can be skipped. There are further optional fields for specfic edge cases, that are described in an extra <code>config.yaml</code> file. In general the <code>config.yaml</code> file consists of a data, an input, an output and a parameter section as well as a few extra fields to define the main benchmark script and benchmark type. Except for the data section the other sections are optional. Multiple values can be parsed as lists.</p> <pre><code># Data section to describe the object and the associated (result) dataset\ndata:\n# Name of the dataset\nname: \"out_dataset\"\n# Title of the dataset (Optional)\ntitle: \"Output of an example OmniObject\"\n# Description of the dataset (Optional)\ndescription: \"This dataset is supposed to store the output files from the example omniobject\"\n# Dataset keyword(s) to make this dataset reachable for other projects/benhcmark components\nkeywords: [\"example_dataset\"]\n# Script to be run by the workflow associated to the project\nscript: \"path/to/method/dataset/metric/script.py\"\n# Interpreter to run the script (Optional, automatic detection)\ninterpreter: \"python\"\n# Benchmark that the object is associated to.\nbenchmark_name: \"omni_celltype\"\n# Orchestrator url of the benchmark (Optional, automatic detection)\norchestrator: \"https://www.orchestrator_url.com\"\n# Input section to describe output file types. (Optional)\ninputs:\n# Keyword to find input datasets, that shall be imported \nkeywords: [\"import_this\", \"import_that\"]\n# Input file types\nfiles: [\"count_file\", \"dim_red_file\"]\n# Prefix (part of the filename is sufficient) to automatically detect file types by their names\nprefix:\ncount_file: \"counts\"\ndim_red_file: [\"features\", \"genes\"]\n# Output section to describe output file types. (Optional)\noutputs:\n# Output filetypes and their endings\nfiles:\ncorrected_counts: end: \".mtx.gz\"\nmeta:\nend: \".json\"\n# Parameter section to describe the parameter dataset, values and filter. (Optional)\nparameter:\n# Names of the parameter to use\nnames: [\"param1\", \"param2\"]\n# Keyword(s) used to import the parameter dataset\nkeywords: [\"param_dataset\"]\n# Filter that specify limits, values or combinations to exclude\nfilter:\nparam1:\nupper: 50\nlower: 3\nexclude: 12\nparam2:\n\"path/to/file/with/parameter/combinations/to/exclude.json\"\n</code></pre> <p>Specific fields, that are only relevant for edge cases. These fields have their counterparts in the generated {ref}<code>OmniObject &lt;section-classes&gt;</code>. Changes of the attributes of the OmniObject instance have the same effects, but come with the flexibility of python code. </p> <pre><code># Command to generate the workflow with (Optional, automatic detection)\ncommand_line: \"python path/to/method/dataset/metric/script.py --count_file data/import_this_dataset/...mtx.gz\"\ninputs:\n# Datasets and manual file type specifications (automatic detection!)\ninput_files:\nimport_this_dataset:\ncount_file: \"data/import_this_dataset/import_this_dataset__counts.mtx.gz\"\ndim_red_file: \"data/import_this_dataset/import_this_dataset__dim_red_file.json\"\n# (Dataset) name that default input files belong to (Optional, automatic detection)\ndefault: \"import_this_dataset\"\n# Input dataset names that should be ignored (even if they have one of the specified input keywords assciated)\nfilter_names: [\"data1\", \"data2\"]\noutputs:\n# Template to automatically generate output filenames (Optional - recommended for advanced user only)\ntemplate: \"data/${name}/${name}_${unique_values}_${out_name}.${out_end}\"\n# Variables used for automatic output filename generation (Optional - recommended for advanced user only)\ntemplate_vars:\nvars1: \"random\"\nvars2: \"variable\"\n# Manual specification of mapping for output files and their corresponding input files and parameter values (automatic detection!)\nfile_mapping:\nmapping1: output_files:\ncorrected_counts: \"data/out_dataset/out_dataset_import_this__param1_10__param2_test_corrected_counts.mtx.gz\"\nmeta: \"data/out_dataset/out_dataset_import_this__param1_10__param2_test_meta.json\"\ninput_files:\ncount_file: \"data/import_this_dataset/import_this_dataset__counts.mtx.gz\"\ndim_red_file: \"data/import_this_dataset/import_this_dataset__dim_red_file.json\"\nparameter:\nparam1: 10\nparam2: \"test\"\n# Default output files (Optional, automatic detection)\ndefault:\ncorrected_counts: \"data/out_dataset/out_dataset_import_this__param1_10__param2_test_corrected_counts.mtx.gz\"\nmeta: \"data/out_dataset/out_dataset_import_this__param1_10__param2_test_meta.json\"\nparameter:\ndefault:\nparam1: 10\nparam2: \"test\"\n</code></pre>"},{"location":"01_getting_started/01_module_contr/project_setup/","title":"Project setup","text":""},{"location":"01_getting_started/01_module_contr/project_setup/#start-a-renku-project","title":"Start a renku project","text":"<p>Each module of omnibenchmark is an independent renku project. It can be setup as a new GitLab project with the following features:</p> <ul> <li>A <code>Dockerfile</code> to specify the computational environment to run your project in.</li> <li>A <code>gitlab-ci.yaml</code> file to continuously build and update the projects Docker container and module script.</li> <li><code>Git LFS</code> for large file storage.</li> <li>Further project organisation files like a <code>.gitignore</code>, <code>.gitattributes</code>, <code>.renkulfsignore</code>, <code>.dockerignore</code>.</li> </ul> <p>Having independent modules also means that you can test your code and work on your project without being included into an existing benchmark. Only when a project is included into an omnibenchmark orchestrator it becomes part of the benchmark itself (explained latter Omnibenchmark modules) .</p> <p>To start a new renku project login at the renku with your GitHub account, OrchID or SWITCH-eduID or register a renku account.</p> <p>Info</p> <p>Start a new renku project: renkulab.io -&gt; projects -&gt; + New project</p> <p>Select a name, namespace, description and suitable template and create a new project. There are no specific requirements for omnibenchmark at this stage.</p> <p>Read more about renku projects here.</p>"},{"location":"01_getting_started/01_module_contr/project_setup/#specify-your-modules-environment","title":"Specify your modules environment","text":"<p>A module environment can be specified by modifying the <code>Dockerfile</code>. To run omnibenchmark you need the python modules renku-python and omnibenchmark installed. There are no further requirements, but it can be useful to follow the template structure in the automatically generated <code>Dockerfile</code>. Depending on what template you chose a renku base image will be selected at the top. Make sure this base image is reasonable for your module (e.g. choose one with R installed if your module calls R code). Extra linux (ubuntu) software requirements can be specified within the <code>Dockerfile</code>, while python modules are typically defined in the <code>requirements.txt</code> file or using conda\u2019s environment management system in the <code>environment.yaml</code> file and installation of R packages is specified in the <code>install.R</code> file. Detailed instruction can be found here. The automatically generated <code>Dockerfile</code> already contains commands to install renku-python at the bottom, but you need to specify omnibenchmark with the version you want to use as software requirement:</p> <p>Info</p> <p>Add this line to requirements.txt: omnibenchmark==VERSION</p> <p>If you want to call  R code and you chose a R template when creating the renku project the <code>install.R</code> file is automatically generated upon project creation. Otherwise make sure you switch to a <code>base image</code> with R installed and add the <code>install.R</code> file manually, as well as the following lines to your <code>Dockerfile</code>:</p> <pre><code># install the R dependencies\nCOPY install.R /tmp/\nRUN R -f /tmp/install.R\n</code></pre>"},{"location":"01_getting_started/01_module_contr/project_setup/#run-your-code","title":"Run your code","text":"<p>After each commit renku builds automatically a Docker container with your specified requirements. You can start an interactive session using the latest container at renkulab.io or work with renku on your own machine.</p>"},{"location":"01_getting_started/01_module_contr/templates/","title":"Start from templates","text":""},{"location":"01_getting_started/01_module_contr/templates/#templates","title":"Templates","text":"<p>Any part of Omnibenchmark can be extended with an additional project that will add a new dataset, method or metric to the evaluation. </p> <p>To ease the addition of a new project to Omnibenchmark, you can select a template when creating a new Renku project. </p> <p>Note</p> <p>Omnibenchmark is built upon the Renku system but the templates are made in a way that you won't have to worry about the underlying Renku mechanisms! You will just have to add your data and/or your code to project that will extend Omnibenchmark. </p>"},{"location":"01_getting_started/01_module_contr/templates/#using-templates-on-renku","title":"Using templates on Renku","text":"<p>The following guide will show you how to create a new Omnibenchmark project for any module (data, method, metric,...).</p> <p>If your benchmark is in the list below, you can click on the module that you would like to add and jump to step 11; </p> <p>Batch-correction Omnibenchmark:      - Data     - Method     - Metric</p> <p>Clustering Omnibenchmark:      - Data     - Method     - Metric</p> <p>Spatial clustering Omnibenchmark:      - Data     - Method     - Metric</p>"},{"location":"01_getting_started/01_module_contr/modules/","title":"Index","text":""},{"location":"01_getting_started/01_module_contr/modules/#omnibenchmark-modules","title":"Omnibenchmark modules","text":"<p>Omnibenchmark uses the Renku platform to run open and continuous benchmarks.  To contribute an independent module to one of the existing benchmarks start by creating a <code>new renku project &lt;https://omnibenchmark.readthedocs.io/en/latest/start/01_project_setup.html#start-a-renku-project&gt;</code>.  Each module consists of a Docker image, that defines its environment, a dataset to store outputs and metadata, a workflow that describes how to generate outputs and input and parameter datasets with input files and parameter definitions, if they are used.  Thus each module is an independent benchmark part and can be run, used and modified independently as such.  Modules are connected by importing (result) datasets from other modules as input datasets and will automatically be updated according to them. All relevant information on how to run a specific module are stored as <code>OmniObject &lt;https://omnibenchmark.readthedocs.io/en/latest/topic_guides/classes/01_omni_object.html&gt;</code>. The most convenient way to generate an instance of an OmniObject is to build it from a <code>config.yaml &lt;https://omnibenchmark.readthedocs.io/en/latest/topic_guides/02_config_yaml.html&gt;</code>_ file `. This structure is universal all modules can be build upon it.  In the following we describe example setups for different benchmark stages (dataset modules, methods modules, metrics modules).  Depending on the benchmark these can be flexibly extended and adapted to the benchmark structure."},{"location":"01_getting_started/01_module_contr/modules/01_data/","title":"Data modules","text":"<p>Data modules are modules that define input datasets and bundle them into a renku dataset, that can be imported by other projects. Benchmark specific requirements like file formats, types and prefixes can be checked at the omnibenchmark webside. Most modules contain 3 main files:</p>"},{"location":"01_getting_started/01_module_contr/modules/01_data/#1-the-configyaml-file","title":"1. The config.yaml file","text":"<p>All information about the module are specified in the {ref}<code>config.yaml file &lt;section-config&gt;</code>:</p> <pre><code>---\ndata:\nname: \"dataset-name\"\ntitle: \"dataset title\"\ndescription: \"A new dataset module for omnibenchmark\"\nkeywords: [\"MODULE_KEY\"]\nscript: \"path/to/module_script\"\noutputs:\nfiles:\ndata_file1: end: \"FILE1_END\"\ndata_file2:\nend: \"FILE2_END\"\ndata_file3:\nend: \"FILE3_END\"\nbenchmark_name: \"OMNIBENCHMARK_TYPE\"\n</code></pre> <p>Entries in capital letters depend on the specifications at the omnibenchmark webside.</p>"},{"location":"01_getting_started/01_module_contr/modules/01_data/#2-the-run_workflowpy-file","title":"2. The run_workflow.py file","text":"<p>This file is to generate, run and update the modules dataset and workflow. A most basic script to do so looks like this:</p> <pre><code># Load modules\nfrom omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml\nfrom omnibenchmark.renku_commands.general import renku_save\n# Build an OmniObject from the config.yaml file\nomni_obj = get_omni_object_from_yaml('src/config.yaml')\n# Create the output dataset\nomni_obj.create_dataset()\nrenku_save()\n## Run and update the workflow\nomni_obj.run_renku()\nrenku_save()\n## Add files to output dataset\nomni_obj.update_result_dataset()\nrenku_save()\n</code></pre>"},{"location":"01_getting_started/01_module_contr/modules/01_data/#3-the-module-script","title":"3. The module script","text":"<p>This is the script to load the dataset and to convert its files into the expected format. Omnibenchmark accepts any kind of script and its maintenance and content is up to the module author. Omnibenchmark calls this script from the command line. If you use another language than R, Python, Julia or bash, specify the interpreter to use in the corresponding field of the {ref}<code>config.yaml file &lt;section-config&gt;</code> file.</p> <p>Note</p> <p>All input and output files and if applicable parameter need to be parsed from the command line in the format: <code>--ARGUMENT_NAME ARGUMENT_VALUE</code></p> <p>In Python argparse can be used to parse command arguments like this:</p> <pre><code># Load module\nimport argparse\n# Get command line arguments and store them in args\nparser=argparse.ArgumentParser()\nparser.add_argument('--argument_name', help='Description of the argument')\nargs=parser.parse_args()\n# Call the argument\narg1 = args.argument_name\n</code></pre> <p>In R we recommend to use the optparse package:</p> <pre><code># Load package\nlibrary(optparse)\n# Get list with command line arguments by name\noption_list = list(\nmake_option(c(\"--argument_name\"), type=\"character\", default=NULL, help=\"Description of the argument\", metavar=\"character\")\n); opt_parser = OptionParser(option_list=option_list);\nopt = parse_args(opt_parser);\n# An useful error if the argument is missing\nif (is.null(opt$argument_name)){\nprint_help(opt_parser)\nstop(\"Argument_name needs to be specified, but is missing.n\", call.=FALSE)\n}\n# Call the argument\narg1 &lt;- opt$argument_name\n</code></pre>"},{"location":"01_getting_started/01_module_contr/modules/02_method/","title":"Method modules","text":"<p>A method module imports all datasets of a benchmark or all preprocessed inputs and runs one benchmarking method on them. Method outputs are added to a renku dataset, that can be imported by metric projects to evaluate them. Benchmark specific requirements like file formats, types and prefixes can be checked at the omnibenchmark webside. To exlore a methods parameter space a parameter dataset is imported. Valid parameter can be specified in the <code>config.yaml file &lt;section-config&gt;</code> or specific {ref}<code>filter &lt;section-filter&gt;</code> files. There are {ref}<code>filter &lt;section-filter&gt;</code> at different level, e.g. parameter limits, specific values and parameter and input file combinations. Usually a method module contains only one workflow, that is automatically run with all valid parameter and input file combinations. Most modules contain 3 main files:</p>"},{"location":"01_getting_started/01_module_contr/modules/02_method/#1-the-configyaml-file","title":"1. The config.yaml file","text":"<p>All information about the module are specified in the {ref}<code>config.yaml file &lt;section-config&gt;</code>:</p> <pre><code>---\ndata:\nname: \"method name\"\ntitle: \"method title\"\ndescription: \"Short description of the method\"\nkeywords: [\"MODULE_KEY\"]\nscript: \"path/to/module_script\"\nbenchmark_name: \"OMNIBENCHMARK_TYPE\"\ninputs:\nkeywords: [\"INPUT_KEY1\", \"INPUT_KEY2\"]\nfiles: [\"input_file_name1\", \"input_file_name2\"]\nprefix:\ninput_file_name1: \"_INPUT1_\"\ninput_file_name2: \"_INPUT2_\"\noutputs:\nfiles:\nmethod_result1: end: \"FILE1_END\"\nmethod_result2:\nend: \"FILE2_END\"\nparameter:\nnames: [\"parameter1\", \"parameter2\"]\nkeywords: [\"PARAMETER_KEY\"]\n</code></pre> <p>Entries in capital letters depend on the specifications at the omnibenchmark webside.</p>"},{"location":"01_getting_started/01_module_contr/modules/02_method/#2-the-run_workflowpy-file","title":"2. The run_workflow.py file","text":"<p>This file is to generate, run and update the modules dataset and workflow. A most basic script to do so looks like this:</p> <pre><code># Load modules\nfrom omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml\nfrom omnibenchmark.renku_commands.general import renku_save\n# Build an OmniObject from the config.yaml file\nomni_obj = get_omni_object_from_yaml('src/config.yaml')\n# Import and update inputs/parameter and update object accordingly \nomni_obj.update_object()\nrenku_save()\n# Create the output dataset\nomni_obj.create_dataset()\nrenku_save()\n## Run and update the workflow on all inputs and parameter combinations\nomni_obj.run_renku()\nrenku_save()\n## Add files to output dataset\nomni_obj.update_result_dataset()\nrenku_save()\n</code></pre>"},{"location":"01_getting_started/01_module_contr/modules/02_method/#3-the-module-script","title":"3. The module script","text":"<p>This is the script to load the dataset and to convert its files into the expected format. Omnibenchmark accepts any kind of script and its maintenance and content is up to the module author. Omnibenchmark calls this script from the command line. If you use another language than R, Python, Julia or bash, specify the interpreter to use in the corresponding field of the {ref}<code>config.yaml file &lt;section-config&gt;</code> file.</p> <p>Note</p> <p>All input and output files and if applicable parameter need to be parsed from the command line in the format: <code>--ARGUMENT_NAME ARGUMENT_VALUE</code></p> <p>In Python argparse can be used to parse command arguments like this:</p> <pre><code># Load module\nimport argparse\n# Get command line arguments and store them in args\nparser=argparse.ArgumentParser()\nparser.add_argument('--argument_name', help='Description of the argument')\nargs=parser.parse_args()\n# Call the argument\narg1 = args.argument_name\n</code></pre> <p>In R we recommend to use the optparse package:</p> <pre><code># Load package\nlibrary(optparse)\n# Get list with command line arguments by name\noption_list = list(\nmake_option(c(\"--argument_name\"), type=\"character\", default=NULL, help=\"Description of the argument\", metavar=\"character\")\n); opt_parser = OptionParser(option_list=option_list);\nopt = parse_args(opt_parser);\n# An useful error if the argument is missing\nif (is.null(opt$argument_name)){\nprint_help(opt_parser)\nstop(\"Argument_name needs to be specified, but is missing.n\", call.=FALSE)\n}\n# Call the argument\narg1 &lt;- opt$argument_name\n</code></pre>"},{"location":"01_getting_started/01_module_contr/modules/03_metric/","title":"Metric modules","text":"<p>A metric module imports method result datasets and runs one evaluation on them. Evaluation results are added to a renku dataset, that can be summarized and explored in an {ref}<code>omnibenchmark bettr dashboard &lt;section-output&gt;</code>. Benchmark specific requirements like file formats, types and prefixes can be checked at the omnibenchmark webside. Usually a metric module contains two workflows, one to evaluate the results and one to generate the <code>metric info file</code>. Most metric modules contain 5 main files:</p>"},{"location":"01_getting_started/01_module_contr/modules/03_metric/#1-the-configyaml-file","title":"1. The config.yaml file","text":"<p>All information about the module are specified in the {ref}<code>config.yaml file &lt;section-config&gt;</code>:</p> <pre><code>---\ndata:\nname: \"metric name\"\ntitle: \"metric title\"\ndescription: \"Short description of the metric\"\nkeywords: [\"MODULE_KEY\"]\nscript: \"path/to/module_script\"\nbenchmark_name: \"OMNIBENCHMARK_TYPE\"\ninputs:\nkeywords: [\"INPUT_KEY1\", \"INPUT_KEY2\"]\nfiles: [\"input_file_name1\", \"input_file_name2\"]\nprefix:\ninput_file_name1: \"_INPUT1_\"\ninput_file_name2: \"_INPUT2_\"\noutputs:\nfiles:\nmetric_result: end: \"FILE1_END\"\n</code></pre> <p>Entries in capital letters depend on the specifications at the omnibenchmark webside.</p>"},{"location":"01_getting_started/01_module_contr/modules/03_metric/#2-the-info_configyaml-file","title":"2. The info_config.yaml file","text":"<p>All information about the module are specified in the {ref}<code>config.yaml file &lt;section-config&gt;</code>:</p> <pre><code>---\ndata:\nname: \"metric name\"\nscript: \"src/generate_metric_info.py\"\nbenchmark_name: \"OMNIBENCHMARK_TYPE\"\noutputs:\nfiles:\nmetric_info: end: \"json\"\nfile_mapping:\nmapping1:\noutput_files:\nmetric_info: \"path/to/metric_name_info.json\"\n</code></pre>"},{"location":"01_getting_started/01_module_contr/modules/03_metric/#3-the-run_workflowpy-file","title":"3. The run_workflow.py file","text":"<p>This file is to generate, run and update the modules dataset and workflow. A most basic script to do so looks like this:</p> <pre><code># Load modules\nfrom omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml\nfrom omnibenchmark.renku_commands.general import renku_save\n# Build an OmniObject from the config.yaml file\nomni_obj = get_omni_object_from_yaml('src/config.yaml')\n# Import and update inputs/parameter and update object accordingly \nomni_obj.update_object()\nrenku_save()\n# Create the output dataset\nomni_obj.create_dataset()\nrenku_save()\n## Run and update the workflow on all inputs and parameter combinations\nomni_obj.run_renku()\nrenku_save()\n## Add files to output dataset\nomni_obj.update_result_dataset()\nrenku_save()\n###################### Generate info file ######################\n# Build an OmniObject from the info_config.yaml file\nomni_info = get_omni_object_from_yaml('src/info_config.yaml')\nomni_info.wflow_name = \"metric_info\"\n## Run and update workflow\nomni_info.run_renku()\nrenku_save()\n## Update output dataset \nomni_info.update_result_dataset()\nrenku_save()\n</code></pre>"},{"location":"01_getting_started/01_module_contr/modules/03_metric/#4-the-module-script","title":"4. The module script","text":"<p>This is the script to load the dataset and to convert its files into the expected format. Omnibenchmark accepts any kind of script and its maintenance and content is up to the module author. Omnibenchmark calls this script from the command line. If you use another language than R, Python, Julia or bash, specify the interpreter to use in the corresponding field of the {ref}<code>config.yaml file &lt;section-config&gt;</code> file.</p> <p>Note</p> <p>All input and output files and if applicable parameter need to be parsed from the command line in the format: <code>--ARGUMENT_NAME ARGUMENT_VALUE</code></p> <p>In Python argparse can be used to parse command arguments like this:</p> <pre><code># Load module\nimport argparse\n# Get command line arguments and store them in args\nparser=argparse.ArgumentParser()\nparser.add_argument('--argument_name', help='Description of the argument')\nargs=parser.parse_args()\n# Call the argument\narg1 = args.argument_name\n</code></pre> <p>In R we recommend to use the optparse package:</p> <pre><code># Load package\nlibrary(optparse)\n# Get list with command line arguments by name\noption_list = list(\nmake_option(c(\"--argument_name\"), type=\"character\", default=NULL, help=\"Description of the argument\", metavar=\"character\")\n); opt_parser = OptionParser(option_list=option_list);\nopt = parse_args(opt_parser);\n# An useful error if the argument is missing\nif (is.null(opt$argument_name)){\nprint_help(opt_parser)\nstop(\"Argument_name needs to be specified, but is missing.n\", call.=FALSE)\n}\n# Call the argument\narg1 &lt;- opt$argument_name\n</code></pre>"},{"location":"01_getting_started/01_module_contr/modules/03_metric/#5-the-generate_metric_infopy-file","title":"5. The generate_metric_info.py file","text":"<p>This file could also be written in R or any other language. It should return a json file with the below fields with the metrics information.</p> <pre><code>import argparse\nimport json\nparser=argparse.ArgumentParser()\nparser.add_argument('--metric_info', help='Path to the metric info json file')\nargs=parser.parse_args()\nmetric_info = {\n'flip': False,\n'max': 1,\n'min': 0,\n'group': \"METRIC_GROUP\",\n'name': \"metric_name\",\n'input': \"metric_input_type\"\n}\nwith open(args.metric_info, \"w\") as fp:\njson.dump(metric_info , fp, indent=3) \n</code></pre>"},{"location":"01_getting_started/01_module_contr/modules/04_submit/","title":"Submit a module to an Omnibenchmark","text":"<p>An omnibenchmark module will be able to import datasets from other modules and export its output to others. However, it still needs to be integrated in an Omnibenchmark orchestrator. An orchestrator is an omnibenchmark project which orchestrates the deployment, running and testing of each pieces of an Omnibenchmark. </p>"},{"location":"01_getting_started/01_module_contr/modules/04_submit/#integrate-a-module-to-an-orchestrator","title":"Integrate a module to an orchestrator","text":"<p>The list of current Omnibenchmarks and their related orchestrator can be found on the Omnibenchmark dashboard: </p> <ul> <li> <p>Omni-batch orchestrator</p> </li> <li> <p>Omni-clustering orchestrator</p> </li> </ul> <p>To integrate your (populated and tested) module: </p> <ul> <li> <p>Follow the link to the relevant orchestrator</p> </li> <li> <p>Open a new issue and describe briefly the aim of your module (data/ method/ metric module ?)</p> </li> <li> <p>The development team will check your module and integrate it in the Orchestrator worfklow. When it is done, you will be able to view the result of your module on the shiny app. </p> </li> </ul>"},{"location":"01_getting_started/02_benchmarker/","title":"Index","text":""},{"location":"01_getting_started/02_benchmarker/#benchmarker","title":"Benchmarker","text":""},{"location":"01_getting_started/02_benchmarker/#project-setup-with-omnibus","title":"Project setup with Omnibus","text":""},{"location":"01_getting_started/02_benchmarker/#add-metadata-information-to-omni_essentials","title":"Add metadata information to omni_essentials","text":"<p>https://github.com/omnibenchmark/omni_essentials/blob/main/general/benchmark_categories.json</p>"},{"location":"01_getting_started/02_benchmarker/#populating-the-benchmark","title":"Populating the benchmark","text":"<p>See <code>Module contributor</code></p>"},{"location":"01_getting_started/02_benchmarker/#setting-up-an-orchestrator","title":"Setting up an orchestrator","text":""},{"location":"01_getting_started/03_method_user/","title":"Benchmark outputs","text":"<p>Available here: http://omnibenchmark.org/p/results/</p>"},{"location":"02_advanced/","title":"Index","text":""},{"location":"02_advanced/#advanced-topics","title":"Advanced topics","text":"<ul> <li> <p>Module contributor: better control inputs, outputs and scripts. </p> </li> <li> <p>Benchmarker: more complex designs and better control how your benchmark is run and populated.</p> </li> <li> <p>Method user: download the results of a benchmark on your computer and wrangle the shiny app to your needs. </p> </li> <li> <p>Administrator: guides to set up nonmandatory omnibenchmark components (i.e. your own gitlab runners, triplestore etc)</p> </li> </ul>"},{"location":"02_advanced/01_module_contr/","title":"Index","text":""},{"location":"02_advanced/01_module_contr/#advances-topics-for-module-contributor","title":"Advances topics for module contributor","text":"<ul> <li>The <code>omni_object</code> instance and its classes</li> </ul>"},{"location":"02_advanced/01_module_contr/omni_object/","title":"Omnibenchmark classes","text":"<p>This section describes classes to manage omnibenchmark modules and their interactions. The main class is the <code>OmniObject</code>, which consollidates all relevant information and functions of a module. This object has further subclasses that define inputs, outputs, commands and the workflow.</p>"},{"location":"02_advanced/01_module_contr/omni_object/classes/01_omni_object/","title":"OmniObject","text":"<p>Main class to manage an omnibenchmark module. It takes the following arguments:</p> <ul> <li><code>name (str)</code>: Module name</li> <li><code>keyword (Optional[List[str]], optional)</code>: Keyword associated to the modules output dataset.</li> <li><code>title (Optional[str], optional)</code>: Title of the modules output dataset.</li> <li><code>description (Optional[str], optional)</code>: Description of the modules output dataset.</li> <li><code>script (Optional[PathLike], optional)</code>: Script to generate the modules workflow for.</li> <li><code>command (Optional[OmniCommand], optional)</code>: Workflow command - will be automatically generated if missing.</li> <li><code>inputs (Optional[OmniInput], optional)</code>: Definitions of the workflow inputs.</li> <li><code>parameter (Optional[OmniParameter], optional)</code>: Definitions of the workflow parameter.</li> <li><code>outputs (Optional[OmniOutput], optional)</code>: Definitions of the workflow outputs.</li> <li><code>omni_plan (Optional[OmniPlan], optional)</code>: The workflow description.</li> <li><code>benchmark_name (Optional[str], optional)</code>: Name of the benchmark the module is associated to.</li> <li><code>orchestrator (Optional[str], optional)</code>: Orchestrator url of the benchmark th emodule is associated to. Automatic detection.</li> <li><code>wflow_name (Optional[str], optional)</code>: Workflow name. Will be set to the module name if none.</li> <li><code>dataset_name (Optional[str], optional)</code>: Dataset name. Will be set to the module name if none.</li> </ul> <p>The following class methods can be run on an instance of an OmniObject:</p> <ul> <li><code>create_dataset()</code>: Method to create a renku dataset with the in the object specified attributes in the current renku project.</li> <li><code>update_object()</code>: Method to check for new imports or updates in input and the parameter datasets. Will update object attributes accordingly.</li> <li><code>run_renku()</code>: Method to generate and update the workflow and all output files as specified in the object.</li> <li><code>update_result_dataset()</code>: Method to update and add all output datasets to the dataset specified in the object.</li> </ul>"},{"location":"02_advanced/01_module_contr/omni_object/classes/02_omni_input/","title":"OmniInput","text":"<p>Class to manage inputs of an omnibenchmark module. This class has the following attributes:</p> <ul> <li><code>names (List[str])</code>: Names of the input filetypes</li> <li><code>prefix (Optional[Mapping[str, List[str]]], optional)</code>: Prefixes (or substrings) of the input filetypes.</li> <li><code>input_files (Optional[Mapping[str, Mapping[str, str]]], optional)</code>: Input files ordered by file types.</li> <li><code>keyword (Optional[List[str]], optional)</code>: Keyword to define which datasets are imported as input datasets.</li> <li><code>default (Optional[str], optional)</code>: Default input name (e.g., dataset).</li> <li><code>filter_names (Optional[List[str]], optional)</code>: Input dataset names to be ignored.</li> </ul> <p>The following class methods can be run on an instance of an OmniInput:</p> <ul> <li><code>update_inputs()</code>: Method to import new and update existing input datasets and update the object accordingly</li> </ul>"},{"location":"02_advanced/01_module_contr/omni_object/classes/03_omni_parameter/","title":"OmniParameter","text":"<p>Class to manage parameter of an omnibenchmark module. This class has the following attributes:</p> <ul> <li><code>names (List[str])</code>: Name of all valid parameter</li> <li><code>values (Optional[Mapping[str, List]], optional)</code>: Parameter values - usually automatically detected.</li> <li><code>default (Optional[Mapping[str, str]], optional)</code>: Default parameter values.</li> <li><code>keyword (Optional[List[str]], optional)</code>: Keyword to import the parameter dataset with.</li> <li><code>filter (Optional[Mapping[str, str]], optional)</code>: Filter to use for the parameter space.</li> <li><code>combinations (Optional[List[Mapping[str, str]]], optional)</code>: All possible parameter combinations.</li> </ul> <p>The following class methods can be run on an instance of an OmniInput:</p> <ul> <li><code>update_parameter()</code>: Method to import and update parameter datasets and update the object/parameter space accordingly.</li> </ul>"},{"location":"02_advanced/01_module_contr/omni_object/classes/04_omni_output/","title":"OmniOutput","text":"<p>Class to manage outputs of an omnibenchmark module. This class has the following attributes:</p> <ul> <li><code>name (str)</code>: Name that is specific for all outputs. Typically the module name/OmniObject name.</li> <li><code>out_names (List[str])</code>: Names of the output file types</li> <li><code>output_end (Optional[Mapping[str, str]], optional)</code>: Endings of the output filetypes.</li> <li><code>out_template (str, optional)</code>: Template to generate output file names.</li> <li><code>file_mapping (Optional[List[OutMapping]], optional)</code>: Mapping of input files, parameter values and output files.</li> <li><code>inputs (Optional[OmniInput], optional)</code>: Object specifying all valid inputs.</li> <li><code>parameter (Optional[OmniParameter], optional)</code>: Object speccifying the parameter space.</li> <li><code>default (Optional[Mapping], optional)</code>: Default output files.</li> <li><code>filter_json(Optional[str], optional)</code>: Path to json file with filter combinations.</li> <li><code>template_fun (Optional[Callable[..., Mapping]], optional)</code>: Function to use to automatically generate output filenames.</li> <li><code>template_vars (Optional[Mapping], optional)</code>: Variables that are used by template_fun.</li> </ul> <p>The following class methods can be run on an instance of an OmniInput:</p> <ul> <li><code>update_outputs()</code>: Method to update the output definitions according to the objects attributes.</li> </ul>"},{"location":"02_advanced/01_module_contr/omni_object/classes/05_omni_command/","title":"OmniCommand","text":"<p>Class to manage the main workflow command of an omnibenchmark module. This class has the following attributes:</p> <ul> <li><code>script (Union[PathLike, str])</code>: Path to the script run by the command</li> <li><code>interpreter (str, optional)</code>: Interpreter to run the script with.</li> <li><code>command_line (str, optional)</code>: Command line to be run.</li> <li><code>outputs (OmniOutput, optional)</code>: Object specifying all outputs.</li> <li><code>input_val (Optional[Mapping], optional)</code>: Input file tyoes and paths to run the command on.</li> <li><code>parameter_val (Optional[Mapping], optional)</code>: Parameter names and values to run the command with.</li> </ul> <p>The following class methods can be run on an instance of an OmniInput:</p> <ul> <li><code>update_command()</code>: Method to update the command according to the outputs,inputs,parameter.</li> </ul>"},{"location":"02_advanced/01_module_contr/omni_object/classes/06_omni_plan/","title":"OmniPlan","text":"<p>Class to manage the workflow of an omnibenchmark module. This class has the following attributes:</p> <ul> <li><code>plan (PlanViewModel)</code>: A plan view model as defined in renku</li> <li><code>param_mapping (Optional[Mapping[str, str]], optional)</code>: A mapping between the component names of the plan and the OmniObject.</li> </ul> <p>The following class methods can be run on an instance of an OmniInput:</p> <ul> <li><code>predict_mapping_from_file_dict()</code>: Method to predict the mapping from the (input-, output-, parameter) file mapping used to generate the command.</li> </ul>"},{"location":"02_advanced/02_benchmarker/","title":"Advanced guides for benchmarker","text":""},{"location":"02_advanced/03_method_user/","title":"Index","text":""},{"location":"02_advanced/03_method_user/#advanced-topics-for-method-user","title":"Advanced topics for method user","text":""},{"location":"02_advanced/04_admin/","title":"Index","text":""},{"location":"02_advanced/04_admin/#administering-omnibenchmark","title":"Administering omnibenchmark","text":""},{"location":"02_advanced/04_admin/#components","title":"Components","text":"<p>Omnibenchmark is designed as a SaaS. Omnibenchmark's components are modular, and some you can deploy in your own to have extra control. omnibenchmark.org already provides all these components, so this guide only matters if you'd like to replace some of the <code>default</code> components.</p> <ul> <li>renkulab and gitlab deployment<ul> <li><code>default</code>: renkulab and gitlab deployment from renkulab.io.</li> <li><code>self</code>: k8s and following renku's admin docs.</li> </ul> </li> <li>gitlab runners <ul> <li><code>default</code>: provided by renkulab.io.</li> <li><code>self</code>: registered runners in any architecture (laptops, HPC, GPU-powered machines etc).</li> </ul> </li> <li>omnibenchmark triplestore<ul> <li><code>default</code>: apache jena from robinsonlab (ask us for details).</li> <li><code>self</code>: deploy a jena/fuseki to have full control on your triples.</li> </ul> </li> <li>centralized benchmark listing/json.<ul> <li><code>default</code>: robinsonlab (ask us for details, queried by omb-py).</li> </ul> </li> <li>bettr deployment <ul> <li><code>default</code>: shiny-server by robinsonlab (ask us for details).</li> <li><code>self</code>: set up a shiny-server (perhaps using singularity).</li> </ul> </li> <li>Web dashboard<ul> <li><code>default</code>: omnibenchmark.org from robinsonlab (ask us for details, code).</li> </ul> </li> </ul> <p>Besides, omnibenchmark relies on a set of python modules and R packages, which also need to be cross-compatible and compatible with your renkulab deployment, including:</p> <ul> <li>omnibenchmark python<ul> <li>tip: needs to be compatible with renku python</li> </ul> </li> <li>omni validator</li> <li>omni_cli</li> <li>bettr</li> </ul>"},{"location":"02_advanced/04_admin/#configuration-guides","title":"Configuration guides","text":"<p>Mind some of our docs are missing; drafts are listed as \u2713, ready-to-use docs as  \u2705.</p> <ol> <li>Services overview  \u2713</li> <li>Start a new benchmark \u2713</li> <li>Set up a triplestore </li> <li>Register a runner \u2705</li> <li>Serve bettr \u2705</li> <li>Serve a dashboard </li> <li>Deploy renkulab </li> </ol>"},{"location":"02_advanced/04_admin/#contact","title":"Contact","text":"<p>To get tokens/authentication details: mark.robinson@uzh.ch, izaskun.mallona@uzh.ch</p>"},{"location":"02_advanced/04_admin/01_services/","title":"Services overview","text":""},{"location":"02_advanced/04_admin/01_services/#services-dependencies","title":"Services dependencies","text":"<p>Several components, mainly omnibenchmark python, query other components (APIs, triplestore) on runtime.</p> <ul> <li>Renku API<ul> <li>omnibenchmark python</li> </ul> </li> <li>Gitlab API<ul> <li>omnibenchmark python</li> <li>gitlab runners</li> </ul> </li> <li>Triplestore<ul> <li>omnibenchmark python (query)</li> <li>omnibenchmark python (population)</li> <li>anywhere (population, from within the CI/CD job; update token needed)</li> <li>(not yet implemented) git/renku hooks</li> </ul> </li> <li>Metric results<ul> <li>a cron job from the machine serving bettr deployments </li> </ul> </li> </ul>"},{"location":"02_advanced/04_admin/01_services/#tips","title":"Tips","text":"<p>You can run renku projects in <code>renku stealth mode</code> disabling the <code>https://renkulab.io/webhooks/events</code> hook within your gitlab project by browsing <code>Settings -&gt; Webhooks</code>.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/","title":"Start a new benchmark","text":"<p>This guide relies on the <code>default</code> omnibenchmark components (gitlab runners, triplestore etc).</p> <p>GUI click/fill instructions have been tested in https://renkulab.io/gitlab running GitLab Community Edition 14.10.5 and might change in future releases.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#foreground","title":"Foreground","text":"<p>The starting point for a non automated benchmark creation is renkulab.io.</p> <p>Interestingly, renkulab.io's gitlab is available at renkulab.io/gitlab. If you log in to renkulab.io you'll be logged in to the gitlab too. To switch easily from renku's GUI to gitlab's GUI please notice the <code>projects</code> or <code>gitlab</code> components of these URLs: </p> <ul> <li>https://renkulab.io/gitlab/omnibenchmark/iris_example/iris-dataset </li> </ul> <p>vs</p> <ul> <li>https://renkulab.io/projects/omnibenchmark/iris_example/iris-dataset</li> </ul> <p>Both refer to the same repository.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#gitlab-group-and-subgroups","title":"Gitlab group (and subgroups)","text":"<p>Repository groups can be created by browsing https://renkulab.io/gitlab pressing <code>new group</code>. Repository subgroups can be created by browsing a group, i.e.  https://renkulab.io/gitlab/omb_benchmarks , and pressing <code>new subgroup</code>. . If interested in creating a subgroup below 'known' omnibenchmark groups such as https://renkulab.io/gitlab/omb_benchmarks  or https://renkulab.io/gitlab/omnibenchmark your user needs to be granted rights; contact the omnibecnhmark team if so.</p> <p>Tip: you can add other people to your benchmark group/subgroup by pressing (left panel) <code>(sub)group information -&gt; Members</code>.</p> <p>Tip: it's advisable to register a dedicated gitlab runner when generating a group, and use it as a group runner for CI/CD. For that, check our runner's docs.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#benchmark-masked-variablestokens","title":"Benchmark masked variables/tokens","text":"<p>Once you've created the group or subgroup where your benchmark will leave (or at least your orchestrator will), you'll need to create a token with <code>api_read</code> scope. For that, visit your group <code>Settings -&gt; CI/CD -&gt; Variables</code> setting and create a nonprotected, masked variable, for instance `OMB_ACCESS_TOKEN~. And keep its content stored somewhere for future usage. </p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#user-tokens","title":"User tokens","text":"<p>A personal gitlab token will allow to automate actions. To generate one: - log in at https://renkulab.io/gitlab - visit https://renkulab.io/gitlab/-/profile/personal_access_tokens - create one with the adequate scope (i.e. read API)</p> <p>This token won't be used by omnibenchmark, but can be handy to have.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#orchestrator","title":"Orchestrator","text":"<p>The core component of omnibenchmark is an orchestrator, which stitches together datasets, methods and metrics. Without an orchestrator there is no benchmark. Still, you can set up the orchestrator last.</p> <p>Orchestrators are unusual omnibenchmark components. They're mainly a gitlab CI/CD yaml. An example orchestrator looks like this:</p> <pre><code>variables:\n  GIT_STRATEGY: fetch\n  GIT_SSL_NO_VERIFY: \"true\"\n  GIT_LFS_SKIP_SMUDGE: 1\n  OMNIBENCHMARK_NAME: iris_example\n  TRIPLESTORE_URL: http://imlspenticton.uzh.ch/omni_iris_data\n  CI_UPSTREAM_PROJECT: ${CI_PROJECT_PATH}\n  OMNI_UPDATE_TOKEN: ${OMNI_UPDATE_TOKEN}\n  CI_PUSH_TOKEN: ${CI_PUSH_TOKEN}\n\nstages:\n  - build\n  - data_run\n  - process_run\n  - parameter_run\n  - method_run\n  - metric_run\n  - summary_metric_run\n\nimage_build:\n  stage: build\n  image: docker:stable\n  rules: \n    - if: '$CI_PIPELINE_SOURCE == \"pipeline\"'\n      when: never\n  before_script:\n    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN http://$CI_REGISTRY\n  script: |\n    CI_COMMIT_SHA_7=$(echo $CI_COMMIT_SHA | cut -c1-7)\n    docker build --tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA_7 .\n    docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA_7\n\ntrigger_iris_dataset:\n  stage: data_run \n  only:\n    - schedules\n  trigger: \n    project: omnibenchmark/iris_example/iris-dataset\n    strategy: depend\n</code></pre> <p>The first stanza (<code>variables</code>) defines variables needed for overall git behaviour as well as helping with authentication. The important tokens are:</p> <ul> <li>OMNI_UPDATE_TOKEN</li> <li>CI_PUSH_TOKEN</li> </ul> <p>The second stanza (<code>image_build</code>) generates a renku-powered docker image (i.e. for interactive sessions).</p> <p>The third stanza (<code>trigger_iris_dataset</code>) is how most of the orchestrator CI/CD tasks look like: they trigger downstream projects CI/CDs; that is, their <code>gitlab-ci.yaml</code>. In the example above, triggers the iris dataset CI/CD.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#terraforming-via-templates","title":"Terraforming via templates","text":"<p>Omnibenchmark's renku templates help to create new benchmark components.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#terraforming-via-gitlab-api","title":"Terraforming via gitlab API","text":"<p>omnibus helps automating benchmark creation.</p>"},{"location":"02_advanced/04_admin/02_new_benchmark/#tipsresources","title":"Tips/resources","text":"<ul> <li>Predefined CI/CD variables</li> </ul>"},{"location":"02_advanced/04_admin/02_new_benchmark/#background","title":"Background","text":"<p>Infrastructure-wise, some manual actions need to be done to start a new benchmark.</p> <ul> <li>register runner(s)</li> <li>set up bettr endpoint</li> <li>add a triplestore dataset</li> <li>add a triplestore apache reverse proxy</li> </ul>"},{"location":"02_advanced/04_admin/02_new_benchmark/#runners","title":"Runners","text":"<p>It's advisable to register a dedicated gitlab runner when generating a benchmark, and use it as a group runner for CI/CD. For that, check our runner's docs</p>"},{"location":"02_advanced/04_admin/03_triplestore/","title":"Set up a triplestore","text":""},{"location":"02_advanced/04_admin/03_triplestore/#setting-up-jenafuseki","title":"Setting up jena/fuseki","text":"<p>Lorem ipsum</p>"},{"location":"02_advanced/04_admin/04_runner/","title":"Register a runner","text":""},{"location":"02_advanced/04_admin/04_runner/#install-gitlab-runner-in-your-linux-machine","title":"Install gitlab-runner in your (linux) machine","text":"<p>For CPU (not GPU) computing, machines to run group gitlab-runners with docker executors need docker and the gitlab-runner software installed. The machine (server, laptop) running the runners does not need a public IP.</p>"},{"location":"02_advanced/04_admin/04_runner/#docker","title":"docker","text":"<p>Assuming your system is <code>apt</code>-based (debian, ubuntu):</p> <pre><code>sudo apt-get update\n\nsudo apt install -y apt-transport-https ca-certificates curl gnupg2 software-properties-common\n\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n\nsudo add-apt-repository \\\n   \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\\n   $(lsb_release -cs) \\\n   stable\"\n\nsudo apt update\n\nsudo apt-get install -y docker-ce docker-ce-cli containerd.io\n\nsystemctl status docker #; start if needed\n\ngroupadd docker\nusermod -aG docker YOURUSER\n</code></pre>"},{"location":"02_advanced/04_admin/04_runner/#gitlab-runner","title":"gitlab-runner","text":"<p>Mind the <code>arch</code>itecture of your machine; amd64 assumed here (Apple M1s: <code>arm64</code>)</p> <pre><code>mkdir -p ~/soft/gitlab-runner; cd $_\n\narch='amd64'\ncurl -LJO \"https://gitlab-runner-downloads.s3.amazonaws.com/latest/deb/gitlab-runner_${arch}.deb\"\nsudo dpkg -i gitlab-runner_amd64.deb\n\nsudo gitlab-runner start\n\n# create an user too\nsudo useradd --comment 'GitLab Runner' --create-home gitlab-runner --shell /bin/bash\n</code></pre>"},{"location":"02_advanced/04_admin/04_runner/#register-a-gitlab-runner-in-your-linux-machine","title":"Register a gitlab runner in your (linux) machine","text":"<p>On GitLab Community Edition 14.10.5 (might sligthly vary within versions) visit your repository or group of repositories, i.e. https://renkulab.io/gitlab/omb_benchmarks, go to <code>CI/CD</code> -&gt; <code>Runners</code> (caution, not to <code>Settings -&gt; CI/CD</code>, click <code>Register a group runner</code>, copy to the clipboard the registration token.</p> <p>Let's assume the token is <code>94daGGiXCgwthisisnotarealtoken</code>.</p> <p>In your gitlab-runner machine, run:</p> <pre><code>REGISTRATION_TOKEN=\"94daGGiXCgwthisisnotarealtoken\"\n\nRUNNER_NAME=examplerunner\nEXECUTOR=\"docker\"\n\nsudo gitlab-runner register \\\n  --non-interactive \\\n  --url \"https://renkulab.io/gitlab/\" \\\n  --registration-token \"$REGISTRATION_TOKEN\" \\\n  --description \"$RUNNER_NAME\" \\\n  -locked=false  \\\n  --run-untagged=true \\\n  --executor \"$EXECUTOR\" \\\n  --description \"$RUNNER_NAME\" \\\n  --docker-image docker:stable \\\n  --docker-network-mode \"host\" \\\n  --docker-volumes /var/run/docker.sock:/var/run/docker.sock\n</code></pre> <p>To check whether the gitlab runner is running,</p> <pre><code>sudo gitlab-runner list\nsudo journalctl -u gitlab-runner\n</code></pre>"},{"location":"02_advanced/04_admin/04_runner/#tips-and-troubleshooting","title":"Tips and troubleshooting","text":"<p>To inspect or edit/finetune the concurrency, timeout behaviour, and/or each runners details can be checked at <code>/etc/gitlab-runner/config.toml</code>. An example config file is:</p> <pre><code>concurrent = 10\ncheck_interval = 0\n\n[session_server]\n  session_timeout = 36000\n\n[[runners]]\n  name = \"tesuto-robinsonlab-gitlab-docker\"\n  url = \"https://renkulab.io/gitlab/\"\n  token = \"YfztssssssssssssssN\"\n  executor = \"docker\"\n  [runners.custom_build_dir]\n  [runners.cache]\n    [runners.cache.s3]\n    [runners.cache.gcs]\n    [runners.cache.azure]\n  [runners.docker]\n    tls_verify = false\n    image = \"docker:stable\"\n    privileged = false\n    disable_entrypoint_overwrite = false\n    oom_kill_disable = false\n    disable_cache = false\n    volumes = [\"/var/run/docker.sock:/var/run/docker.sock\", \"/cache\"]\n    network_mode = \"host\"\n    shm_size = 0\n\n[[runners]]\n  name = \"iris-tesuto-robinsonlab-gitlab-docker\"\n  url = \"https://renkulab.io/gitlab/\"\n  token = \"xbxxxxxxxxxxxxxKi\"\n  executor = \"docker\"\n  [runners.custom_build_dir]\n  [runners.cache]\n    [runners.cache.s3]\n    [runners.cache.gcs]\n    [runners.cache.azure]\n  [runners.docker]\n    tls_verify = false\n    image = \"docker:stable\"\n    privileged = false\n    disable_entrypoint_overwrite = false\n    oom_kill_disable = false\n    disable_cache = false\n    volumes = [\"/var/run/docker.sock:/var/run/docker.sock\", \"/cache\"]\n    network_mode = \"host\"\n    shm_size = 0\n</code></pre> <p>Double check <code>privileged = false</code>, needs to be false. Re: the OOM killer, TLS verify etc: up to you.</p> <p><code>concurrent</code> limits how many jobs can run concurrently, across all registered runners; beware gitlab-runner does not know how many cores does each job use. For mostly single-core jobs defining <code>concurrent</code> as your machine's <code>nproc</code> - 2 should work.</p> <p>Very useful advanced config.toml docs.</p> <p>Mind that your gitlab repositories list their registered runners at <code>Settings -&gt; CI/CD -&gt; Runners</code>. Beware of the <code>shared runners</code>, <code>group runners</code> (the ones we're configuring here) and <code>other available runners</code>; disable the ones you don't want to use. </p> <p>Shell executors are also easy to config. Docker-in-docker is not working well due to subpar caching.</p> <p>Remember to remove old images and vacuum/clean your machine (with a cron job). A recipe to prune old images, containers, networks and volumes (to be cron-ed at midnight):</p> <pre><code>#!/bin/bash\n##\n## Prunes images, containers, networks, volumes (ideally daily, at midnight)\n## Does some convoluted checks to keep the most up-to-date image\n## largely untested\n##\n## 18 Aug 2022\n## Izaskun Mallona\n## GPLv3\n\n# prune images older than 24h\n## the -a means it removes dangling but also those not used by existing containers\n# the until-24h means removes those older than 24h\n\ndocker network prune  --filter \"until=200h\" -f\n\ndocker volume prune  -f \"until=200h\"\n\nfor diru in $(docker images -a --format \"{{.Repository}}\" | sort | uniq)\ndo\n    ## sort by the timestamp, list all except the first/most recent\n    for old_thing in $(docker images -a --format \\\n        \"{{.ID}}\\t{{.Size}}\\t{{.Repository}}\\t{{.CreatedAt}}\" | \\\n        grep $diru | \\\n        sort -k4r | \\\n        tail -n+2 | \\\n        cut -f1)\n    do\n        echo Removing $old_thing from $diru\n        docker rmi $old_thing;\n    done\ndone\n\n#remove dangling\ndocker image prune -f\n</code></pre>"},{"location":"02_advanced/04_admin/05_bettr/","title":"Serve bettr","text":"<p>We are currently serving bettr via shiny-server within singularity. So this recipe aims to install singularity on a linux machine, get the bettr image, and set up a cron to retrieve metrics.</p> <p>The host needs to face the internet. And a firewall.</p> <p>We are aware rstudioconnect could simplify this, or just running <code>bettr</code> locally.</p>"},{"location":"02_advanced/04_admin/05_bettr/#installs","title":"Installs","text":""},{"location":"02_advanced/04_admin/05_bettr/#singularity","title":"singularity","text":"<p>We assume an <code>apt</code>-apt distribution, i.e. debian or ubuntu. <pre><code>sudo apt-get update\n\nsudo apt-get update &amp;&amp; sudo apt-get install -y \\\n    build-essential \\\n    uuid-dev \\\n    libgpgme-dev \\\n    squashfs-tools \\\n    libseccomp-dev \\\n    wget \\\n    pkg-config \\\n    git \\\n    cryptsetup-bin \\\n    libgpgme11-dev\n\nmkdir -p ~/soft/go\ncd $_\n\nexport VERSION=1.13.5 OS=linux ARCH=amd64 &amp;&amp; \\\n    wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz\n\nsudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz\n\necho 'export GOPATH=${HOME}/go' &gt;&gt; ~/.bashrc &amp;&amp; \\\n    echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' &gt;&gt; ~/.bashrc &amp;&amp; \\\n    source ~/.bashrc\n\nmkdir -p ~/soft/singularity\ncd $_\n\nexport VERSION=3.8.3 &amp;&amp; # adjust this as necessary \\\n    wget https://github.com/sylabs/singularity/releases/download/v3.8.3/singularity-ce-3.8.3.tar.gz\n\ntar -xzf singularity-*${VERSION}.tar.gz\n\ncd sing*\n\n./mconfig &amp;&amp; \\\n    make -C ./builddir\n\nsudo make -C ./builddir install\n\n# test\n\nsingularity exec library://alpine cat /etc/alpine-release\n</code></pre></p>"},{"location":"02_advanced/04_admin/05_bettr/#apache","title":"apache","text":"<pre><code>sudo apt install apache2\n</code></pre> <p>Also, configure iptables. Or ufw, open port 80/443, 22, and whatever the port the bettr image is going to use below.</p>"},{"location":"02_advanced/04_admin/05_bettr/#bettr-image","title":"bettr image","text":"<p>The bettr image is generated by https://renkulab.io/gitlab/omnibenchmark/bettr-deployer .</p>"},{"location":"02_advanced/04_admin/05_bettr/#serving-the-bettr-app","title":"serving the bettr app","text":"<p>To read the bettr image from the registry, an user token with <code>read_registry</code> power is needed; below encoded as <code>INGULARITY_DOCKER_PASSWORD</code>.</p> <pre><code>mkdir -p ~/bettr_deployer/apps ~/bettr_deployer/logs ~/bettr_deployer/lib ~/bettr_deployer/tmp\n\ncd ~/bettr_deployer\nexport SINGULARITY_DOCKER_USERNAME='izaskun.xx@xxxuzh.ch'\nexport SINGULARITY_DOCKER_PASSWORD='KxasgasgasgxK' # read_registry granted\nexport NAMESPACE=\"omnibenchmark\"\nexport ID=\"obm_bettr\"\nexport VERSION=\"1d0b31b\"\n\nsingularity instance start --env SHINY_LOG_STDERR=1 \\\n            --bind ./apps:/srv/shiny-server/bettr \\\n            --bind ./log:/var/log/shiny-server_bettr \\\n            --bind ./lib:/var/lib/shiny-server \\\n            --bind ./tmp:/tmp \\\n            bettr-deployer_\"$ID\"-\"$VERSION\".sif \"$ID\"_\"$VERSION\"\n\nsingularity exec instance://\"$ID\"_\"$VERSION\" \"shiny-server\" &amp;\n</code></pre> <p>So apps will be served if placed at ~/bettr_deployer/apps, i.e.:</p> <pre><code>/home/shiny/bettr_deployer/apps/omni_clustering/:\ntotal 12\n-rw-rw-r-- 1 shiny shiny 1614 Jul 27 13:28 app.R\ndrwxrwxr-x 2 shiny shiny 4096 Sep  1  2022 data\n-rw-rw-r-- 1 shiny shiny    0 Sep  1  2022 restart.txt\n\n/home/shiny/bettr_deployer/apps/omni_clustering/data:\ntotal 340\n-rw-rw-r-- 1 shiny shiny 347951 Jan 19  2023 summary.json\n</code></pre> <p>where <code>app.R</code> contains</p> <pre><code>#!/usr/bin/env R\n\n## Retrieve data\n\n## make recognize the argument as the data that has been just downloaded, for FAIR\nsuppressPackageStartupMessages({\n  library(\"bettr\")\n})\n\nresDir   &lt;- 'data'\nbstheme  &lt;- 'darkly'\nappTitle &lt;- 'bettr'\nmetrics  &lt;- 'all'\n\nres_files &lt;- list.files(path = paste0(resDir), full.names = TRUE)\n\n## Read result files\nout &lt;- jsonlite::read_json(res_files, simplifyVector = TRUE)\n\n## reconverting true false to logical\ncolnames(out$metricInfo) &lt;- c(\"Metric\", \"Group\")\nout$initialTransforms &lt;- lapply(out$initialTransforms, function(x){\n  lapply(x, function(y){\n    as.logical(y)\n  })\n})\n\n# replacing na value\nout$idInfo[is.na(out$idInfo)] &lt;- \"NaN\"\n\n# call bettr\nbettr(\n  df = out$df,\n  idCol = if (is.null(out$idCol)) {\n    \"Method\"\n  } else {\n    out$idCol\n  },\n  metrics = if (is.null(out$metrics)){\n    setdiff(colnames(out$df), out$idCol)\n  } else {\n    out$metrics\n  },\n  initialTransforms = if (is.null(out$initialTransforms)) {\n    list()\n  } else {\n    out$initialTransforms\n  },\n  metricInfo = out$metricInfo,\n  metricColors = out$metricColors,\n  idInfo = out$idInfo,\n  idColors = out$idColors,\n  weightResolution = if (is.null(out$weightResolution)) {\n    0.05\n  } else {\n    out$weightResolution\n  },\n  bstheme = if (is.null(out$bstheme)) {\n    \"darkly\"\n  } else {\n    out$bstheme\n  },\n  appTitle = if (is.null(out$appTitle)) {\n    \"bettr\"\n  } else {\n    out$appTitle\n</code></pre> <p>And the data is either a fixed snapshot (or retrieved periodically via cron) of a suitably formated metrics file, like this one.</p>"},{"location":"02_advanced/04_admin/05_bettr/#dmz-related-maintenance","title":"DMZ-related maintenance","text":""},{"location":"02_advanced/04_admin/05_bettr/#firewall-shiny-ports","title":"firewall / shiny ports","text":"<p>Mind to open port 3840 to fit the docker container shiny-server's port, or update the Dockerfile or the singularity port mapping as needed.</p>"},{"location":"02_advanced/04_admin/05_bettr/#cron-to-retrieve-results","title":"cron to retrieve results","text":"<p>Using the clustering benchmark as an example: </p> <pre><code>cd /home/shiny/bettr_deployer/apps/omni_clustering/data\n\nwget -q https://renkulab.io/gitlab/omnibenchmark/omni_clustering/omni_clustering_summary/-/raw/main/data/omni_clustering_summary/omni_clustering_summary.json -O summary.json\n\n## to make sure the shiny app reloads\ntouch ../app.R\n</code></pre>"},{"location":"02_advanced/04_admin/05_bettr/#tips-and-troubleshooting","title":"Tips and troubleshooting","text":"<ul> <li>(internal) The unusual shiny's UID 998 avoids a collision with robinsonlab's 999, whic is the rstudio-server user.</li> </ul>"},{"location":"02_advanced/04_admin/06_dashboard/","title":"Serve the dashboard","text":"<p>Lorem ipsum.</p>"},{"location":"02_advanced/04_admin/07_renkulab/","title":"Deploy renkulab","text":"<p>Lorem ipsum.</p>"},{"location":"03_howto/","title":"Index","text":""},{"location":"03_howto/#how-tos","title":"How to's","text":""},{"location":"03_howto/#build-object","title":"Build object","text":""},{"location":"03_howto/#create-dataset","title":"Create dataset","text":""},{"location":"03_howto/#generate-workflow","title":"Generate workflow","text":""},{"location":"03_howto/#update-object","title":"Update object","text":""},{"location":"03_howto/#filter","title":"Filter","text":""},{"location":"03_howto/01_build_object/","title":"Build object","text":"<p>(section-build-yaml)=</p>"},{"location":"03_howto/01_build_object/#build-an-omniobject-from-yaml","title":"Build an OmniObject from yaml","text":"<p>All relevant information on how to run a specific module are stored as {ref}<code>OmniObject &lt;section-omniobject&gt;</code>. The most convenient way to generate an instance of an <code>OmniObject</code> is to build it from a <code>config.yaml</code> file using the <code>get_omni_object_from_yaml()</code> function:</p> <pre><code>## modules\nfrom omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml\n## Load object\nomni_obj = get_omni_object_from_yaml('src/config.yaml')\n</code></pre>"},{"location":"03_howto/02_create_dataset/","title":"Create dataset","text":"<p>(section-datasets)=</p>"},{"location":"03_howto/02_create_dataset/#datasets","title":"Datasets","text":""},{"location":"03_howto/02_create_dataset/#generate-datasets","title":"Generate Datasets","text":""},{"location":"03_howto/02_create_dataset/#add-files","title":"Add files","text":""},{"location":"03_howto/02_create_dataset/#update-datasets","title":"Update datasets","text":""},{"location":"03_howto/03_generate_workflow/","title":"Generate workflow","text":"<p>(section-workflow)=</p>"},{"location":"03_howto/03_generate_workflow/#generate-and-update-workflows","title":"Generate and update workflows","text":""},{"location":"03_howto/04_update_object/","title":"Update object","text":"<p>(section-update)=</p>"},{"location":"03_howto/04_update_object/#update-omniobject","title":"Update OmniObject","text":""},{"location":"03_howto/05_filter/","title":"Filter Inputs and Parameter to be excluded","text":"<p>Some methods are not applicable for all input datasets or the entire parameter range. Maybe the parameter range to use depends on features of the input dataset. <code>omnibenchmark</code> allows filtering (filter out) at different level to flexibly specify input data bundles to exclude, parameter limits and input-parameter combinations to ignore. See below for different types of filter with examples.</p> <p>Note</p> <p>In general <code>omnibenchmark</code> allows detailed specification of the accepted inputs and parameter to include, e.g., through keywords or explicit definition. Especially keyword-based specification is limited by it's extend to generalize for new (not yet included inputs/parameter). Filtering inputs and outputs to exclude allows automatic and programmatic specification to extend existing generalizations.     </p>"},{"location":"03_howto/05_filter/#filter-explicit-input-data-bundles","title":"Filter explicit input data bundles","text":"<p>While input data bundles with a certain keyword should typically fullfill all requirements of downstream modules, there can be module-specifc requirements, which are violated by single upstream data bundles. A possible solution is to explicitly exclude certain data bundles by name as inputs. These data bundles will automatically be ignored upon data bundle import and input - output file mapping. Input filtering can be specified within the <code>config.yaml</code> or by using attributes of the OmniObject in <code>run_workflow.py</code>:</p> config.yamlrun_workflow.py <pre><code>---\ndata:\n...\nscript: \"src/run_method_XY.R\"\nbenchmark_name: \"omniexample\"\ninputs:\nkeywords: [\"dataset_omniexample\"]\nfiles: [\"counts\", \"meta_file\"]\nprefix:\ncounts: \"_counts\"\nmeta_file: \"_meta\"\nfilter_names: [\"dataset1\", \"dataset2\"] # (1)!\noutputs:\n...\n</code></pre> <ol> <li>Names must correspond to data bundle names (also called slug in renku terminology). A single name can be provided as string and multiple names as list of strings.</li> </ol> <pre><code>import omnibenchmark as omni\n# Generate object from yaml file\nomni_obj = omni.utils.get_omni_object_from_yaml(\"config.yaml\")\n# Explicitly ignore input data bundles by name\nomni_obj.inputs.filter_names = [\"dataset1\", 'dataset2']\n# Update object\nomni_obj.update_object() # (1)!\nrenku_save()\n</code></pre> <ol> <li>Add filter attributes before updating the object to prevent data bundles you want to ignore from being imported. </li> </ol> <p>Note</p> <p>Consider using different input data bundle keywords, if the number of data bundles to explicitly exclude is large or multiple downstream modules have the same restrictions.</p>"},{"location":"03_howto/05_filter/#filter-parameter","title":"Filter parameter","text":"<p>Parameter ranges can be explicitly specified within the <code>config.yaml</code> file. If the global parameter space (i.e., a data bundle with the benchmark-specific parameter space) is used, specific limits or values to exclude can be specified within the <code>config.yaml</code> file or as attributes of the OmniObject in <code>run_workflow.py</code>:</p>"},{"location":"03_howto/05_filter/#parameter-limits","title":"Parameter limits","text":"<p>Limits can only be used to specify ranges of parameter with numeric values. Limits for each parameter are specified separately.  The keys <code>upper</code> and <code>lower</code> are fixed terms.</p> config.yamlrun_workflow.py <pre><code>---\ndata:\n...\nscript: \"src/run_method_XY.R\"\nbenchmark_name: \"omniexample\"\ninputs:\n...\noutputs:\n...\nparameter:\nnames: [\"dims\", \"mode\"]\nkeywords: [\"parameter_omniexample\"]\nfilter: # (1)!\ndims:\nupper: 100\nlower: 10\n</code></pre> <ol> <li>Limits can only be specified for parameter with numeric values. The keys <code>upper</code> and <code>lower</code> are fixed and required.</li> </ol> <pre><code>import omnibenchmark as omni\n# Generate object from yaml file\nomni_obj = omni.utils.get_omni_object_from_yaml(\"config.yaml\")\n# Set parameter limits # (1)!\nomni_obj.parameter.filter = {\n\"dims\": {\n\"lower\": 10, \n\"upper\": 100\n}\n}\n# Update object\nomni_obj.update_object() # (2)!\nrenku_save()\n</code></pre> <ol> <li>Limits can only be specified for parameter with numeric values. The keys <code>upper</code> and <code>lower</code> are fixed and required. </li> <li>Add filter attributes before updating the object to apply filter to the generated outputs. </li> </ol>"},{"location":"03_howto/05_filter/#parameter-values-to-exclude","title":"Parameter values to exclude","text":"<p>Specific values can be ignored/excluded for all inputs. Parameter values to exclude are specified explicitly for each parameter.  The key <code>exclude</code> is a fixed term.</p> config.yamlrun_workflow.py <pre><code>---\ndata:\n...\nscript: \"src/run_method_XY.R\"\nbenchmark_name: \"omniexample\"\ninputs:\n...\noutputs:\n...\nparameter:\nnames: [\"dims\", \"mode\"]\nkeywords: [\"parameter_omniexample\"]\nfilter: # (1)!\nmode:\nexclude: [\"auto\", \"solo\"]\n</code></pre> <ol> <li>Values to exclude can be specified for parameter with numeric and character values. The key <code>exclude</code> is fixed and required.</li> </ol> <pre><code>import omnibenchmark as omni\n# Generate object from yaml file\nomni_obj = omni.utils.get_omni_object_from_yaml(\"config.yaml\")\n# Set parameter to exclude # (1)!\nomni_obj.parameter.filter = {\n\"mode\": {\n\"exclude\": [\"auto\", \"mode\"]\n}\n}\n# Update object\nomni_obj.update_object() # (2)!\nrenku_save()\n</code></pre> <ol> <li>Values to exclude can be specified for parameter with numeric and character values. The key <code>exclude</code> is fixed and required. </li> <li>Add filter attributes before updating the object to apply filter to the generated outputs. </li> </ol>"},{"location":"03_howto/05_filter/#filter-explicit-input-parameter-combinations","title":"Filter explicit input-parameter combinations","text":"<p>Explicit input parameter combinations can be filtered by providing a <code>.json</code> file explicitly listing the input - parameter combinations (mappings) to be ignored and not used for output generation. </p> filter_combinations.json<pre><code>[\n{\n\"input_files\": {\n\"in_file1\": \"data/dataset1/dataset1_in_file1.mtx.gz\",\n\"in_file2\": \"data/dataset1/dataset1_in_file2.json\",\n},\n\"parameter\": {\n\"dims\": 10,\n\"mode\": \"auto\"\n}\n}\n]\n</code></pre> <p>The <code>.json</code> file with explicit mappings to ignore can be specified in the <code>config.yaml</code> file or as attributes of the OmniObject in <code>run_workflow.py</code>:</p> config.yamlrun_workflow.py <pre><code>---\ndata:\n...\nscript: \"src/run_method_XY.R\"\nbenchmark_name: \"omniexample\"\ninputs:\n...\noutputs:\nfiles:\nmethod_res: end: \".json\"\nfilter_json: \"path/to/filter_combinations.json\"\nparameter:\n...\n</code></pre> <pre><code>import omnibenchmark as omni\n# Generate object from yaml file\nomni_obj = omni.utils.get_omni_object_from_yaml(\"config.yaml\")\n# Path to filter combinations.json \nomni_obj.outputs.filter_json = \"src/filter_comb.json\"\n# Update object\nomni_obj.update_object() # (1)!\nrenku_save()\n</code></pre> <ol> <li>Add filter attributes before updating the object to apply filter to the generated outputs. </li> </ol>"},{"location":"03_howto/05_filter/#programmatic-filter","title":"Programmatic filter","text":"<p>If possible any filter should be specified programmatically to allow generalization for new (not yet known) inputs or parameter space extension. We can use the above-mentioned <code>.json</code> file from explicitly filtering input-parameter combinations to automatically filter input data bundles, parameter values or input-parameter combinations.</p> <p>Note</p> <p>Programmatic filtering can not be done on the <code>config.yaml</code> level, but needs to be implemented within <code>run_workflow.py</code>.</p> <ol> <li> <p>Define a filter function describing your filter in a general way.</p> Example <p>The following example uses the <code>omni_obj</code> as input argument, gets the true dimensionality of the data from the input file with the key <code>meta</code> and selects values from the <code>dims</code> parameter that differ more than 3 from the true dimensionality for filtering.</p> filter_function.py<pre><code>import json\nimport pandas as pd\ndef get_param_filter_by_ground_truth(omni_obj):\nfilter_list = []\n# Loop through all input file groups\nfor infile_mapping in omni_obj.inputs.input_files.values():\n# Load meta data from object\nwith open(infile_mapping[\"meta_file\"]) as f:  \nmeta = json.load(f)\n# Get true dim\ndf = pd.DataFrame.from_dict(meta)\nk = meta.get(\"true_dim\")\n# Get parameter combinations to filter\nparam_comb = [\n{param_nam: param_val for param_nam, param_val in param_map.items()}\nfor param_map in omni_obj.parameter.combinations\nif param_map[\"k\"] &lt; k - 3 or param_map[\"k\"] &gt; k + 3\n]\n# Generate filter items\nfilter_list_dat = [\n{\"input_files\": infile_mapping, \"parameter\": param_com}\nfor param_com in param_comb\n]\n# Add to filter_list\nfilter_list = filter_list + filter_list_dat\nreturn filter_list\n</code></pre> </li> <li> <p>Store filtered combinations as <code>.json</code> and assign this as output filter <code>.json</code> file</p> run_workflow.py<pre><code>from omnibenchmark.utils.build_omni_object import get_omni_object_from_yaml\nfrom omnibenchmark.renku_commands.general import renku_save\nimport json\nimport filter_function # (1)!\n## Load config\nomni_obj = get_omni_object_from_yaml('src/config.yaml')\n## Update object and download input datasets\nomni_obj.update_object()\nrenku_save()\n## Generate json with filter combinations\nfilter_comb = filter_function.get_param_filter_by_ground_truth(omni_obj) # (2)!\nwith open(\"filter_comb.json\", \"w\") as fp:\njson.dump(filter_comb, fp, indent=3)\nrenku_save()\n</code></pre> <ol> <li>Import filter function from Step 1</li> <li>See example above (adapt function name and input) </li> </ol> </li> <li> <p>Update outputs and command to apply filter</p> run_workflow.py<pre><code>## update outputs and commands\nomni_obj.outputs.filter_json = \"src/filter_comb.json\" # (1)!\nomni_obj.outputs.update_outputs()\nomni_obj.command.outputs = omni_obj.outputs\nomni_obj.command.update_command()\n## Run workflow\nomni_obj.run_renku()\nrenku_save()\n</code></pre> <ol> <li>Updating outputs and command only (instead of the entire object) to prevent input import/update.</li> </ol> </li> </ol>"},{"location":"04_bugs/","title":"Index","text":""},{"location":"04_bugs/#common-issues","title":"Common issues","text":"<p>List of common issues and their solutions. </p>"},{"location":"04_bugs/01_templates/","title":"Templates project creation","text":""},{"location":"04_bugs/01_templates/#issues-related-to-templates-and-project-creation","title":"Issues related to templates and project creation","text":"<p>More to come...</p>"},{"location":"04_bugs/02_python/","title":"Issues related to Python CLIs","text":""},{"location":"04_bugs/02_python/#-dirtyrepository-the-repository-is-dirty-please-use-the-git-command-to-clean-it","title":"- <code>DirtyRepository: The repository is dirty. Please use the \"git\" command to clean it.</code>","text":"<p>Context: a renku/ omnibenchmark related command is run and returns an exit status with the above message.</p> <p>Explanation: Renku (and Omnibenchmark) commands can only be run when there is no unsaved work.</p> <p>Solution: run <code>renku_save()</code> (Python) or <code>renku save</code> (Bash) and rerun your command.</p>"},{"location":"04_bugs/02_python/#issues-related-to-omni_objupdate_objectalltrue","title":"Issues related to <code>omni_obj.update_object(all=True)</code>","text":"<p>Context: an error is raised when running the above command and no input data is imported. </p> <p>Explanation: there might be different reasons; the orchestrator is not set up, there is no input data,...</p> <p>Solutions: depending on the problem, try the followings: </p> <ul> <li>check that an orchestrator is set up for your Omnibenchmark: https://github.com/omnibenchmark/omni_essentials/blob/main/general/benchmark_categories.json</li> </ul>"},{"location":"04_bugs/02_python/#-omni_objupdate_result_dataset-parametererror-invalid-parameter-value-these-datasets-dont-exist-or-omni_objcreate_dataset-dataset-zhengmix4eq-filterhvg-pinned-already-taken-please-use-a-different-name","title":"- <code>omni_obj.update_result_dataset()</code> &gt; <code>ParameterError: Invalid parameter value - These datasets don't exist:</code> OR <code>omni_obj.create_dataset()</code> &gt; <code>Dataset zhengmix4eq-filterhvg-pinned already taken. Please use a different name.</code>","text":"<p>Context: Error or warning message when creating a dataset in an Omnibenchmark project</p> <p>Explanation: the name is already taken or has conflicting dataset name. Typically, a subset of another name that our name matching algorithms could mix. Example: \"dataset\" vs \"dataset-updated\". </p> <p>Solution: change the <code>name:</code> in your <code>config.yml</code> to resolve the conflict; try to avoid spaces and <code>-</code> and rather use <code>_</code> and more descriptive names.</p>"},{"location":"04_bugs/02_python/#-giterror-cannot-commit-changes","title":"- <code>GitError: Cannot commit changes</code>","text":"<p>Context: error when running a workflow. </p> <p>Explanation: It is possible that some files were generated in your <code>data</code> folder, where the output of your workflow will be moved. For reproducibility and tracing purpose, Renku doesn't allow files generation into your output <code>data</code> folder, if they were not generated by a Renku workflow. </p> <p>Solution: remove the problematic files from your <code>data</code> folder and run again your workflow command. </p>"},{"location":"04_bugs/02_python/#-filenotfounderror-errno-2-no-such-file-or-directory-optcondalibpython39site-packagesomnivalidatorschemas","title":"- <code>FileNotFoundError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/omniValidator/schemas/...'</code>","text":"<p>Explanation: you are using an older version of <code>omniValidator</code>. </p> <p>Solution: upgrade <code>omniValidator</code> to &gt; 0.0.19</p>"},{"location":"04_bugs/02_python/#-interpretererror-interpreter-could-not-be-identified-from-extention-please-specify-command-explicitly","title":"- <code>InterpreterError: Interpreter could not be identified from extention . Please specify command explicitly.</code>","text":"<p>Context: error when calling <code>get_omni_object_from_yaml('src/config.yaml')</code></p> <p>Explanation: your main script likely lacks a complete name (e.g. <code>.R</code>, <code>.py</code>).</p> <p>Solution: give an explicit name to your script (e.g. <code>run_method.R</code>) and to your <code>config.yaml</code> file (<code>script:</code> field). </p>"},{"location":"04_bugs/02_python/#-one-or-several-of-my-data-files-are-only-3-lines-long-but-the-initial-content-is-not-here","title":"- One or several of my data files are only 3 lines long but the initial content is not here","text":"<p>Context: one or several of your data files contain a structure of type: </p> <pre><code>version https://git-lfs.github.com/spec/v1\noid sha256:XXXXXXXXXXXXXXXXXXXXXXXXXXXX\nsize XXXX\n</code></pre> <p>Explanation: one or several of your data files have been stored on Git LFS. By default, all large files are pushed there to save memory. More information on the renku documentation.</p> <p>Solution: on an interactive session, check the <code>fetch automatically LFS data</code> option. In a terminal, you can also fetch the data with <code>git lfs pull</code>. </p>"},{"location":"04_bugs/03_renku_interface/","title":"Renku interface","text":""},{"location":"04_bugs/03_renku_interface/#issues-related-to-renku-website-and-interface","title":"Issues related to Renku website and interface","text":"<p>More to come...</p>"}]}